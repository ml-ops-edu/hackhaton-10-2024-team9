{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc501ac6-a893-466c-9e97-60622400606c",
   "metadata": {},
   "source": [
    "Install the duckdb package for python, and boto3 for s3 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaacfde4-9761-4f2d-b39b-96fe7bbce1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckdb boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed1e30-bf46-47cf-9d9f-19a92f5eafeb",
   "metadata": {},
   "source": [
    "Import the packages and load the configuration.\n",
    "\n",
    "The _creds_ configuration contains your S3 secrets, you must provide them (in ~/.ssh/s3.ini in this example):\n",
    "```\n",
    "[default]\n",
    "key=your S3 keyid\n",
    "secret=your S3 secret\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9443b391-4032-40e0-93c9-3e90be298672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import boto3\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "creds = configparser.ConfigParser()\n",
    "creds.read(os.getenv('HOME') + '/.ssh/s3.ini')\n",
    "config = configparser.RawConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff067db-59fe-48b6-991d-dea3b8ba729e",
   "metadata": {},
   "source": [
    "At the moment DuckDB does not have a built-in catalog concept. To read Iceberg tables from S3, it is necessary to manually retrieve the Iceberg metadata. We define a helper function _get_metadata_ to identify the most recent metadata of an Iceberg table stored on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5da463-bab6-4e7b-b35a-00149e7b4b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(bucket, prefix, s3endpoint, s3key, s3secret):\n",
    "    s3 = boto3.client('s3', endpoint_url='https://'+s3endpoint, aws_access_key_id=s3key, aws_secret_access_key=s3secret)\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix.strip('/') + '/metadata')\n",
    "    latest_manifest = None\n",
    "    latest_time = None\n",
    "    for obj in response.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        if key.endswith('.json'):  # Check for manifest files\n",
    "            last_modified = obj['LastModified']\n",
    "            if latest_time is None or last_modified > latest_time:\n",
    "                latest_time = last_modified\n",
    "                latest_manifest = key\n",
    "    s3.close()\n",
    "    return latest_manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a41d3-c1ef-486f-ad6e-70ca781dd977",
   "metadata": {},
   "source": [
    "Find the metadata paths of all the tables needed for our query, and create a duckdb connection configured for iceberg on S3.\n",
    "Duckdb does not have a concept of catalog like Trino has, but it should in theory(*) be possible to connect to multiple S3 using [secret scopes](https://duckdb.org/docs/configuration/secrets_manager.html#creating-multiple-secrets-for-the-same-service-type). The downside is that the credentials to all the individual sources must be managed by the user, which is not easily portable.\n",
    "\n",
    "(*) As of duckdb v1.1.3 _SCOPE_ not recognized by _iceberg_scan_ on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f86a75-f748-4308-bcfc-d90833d96ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefixes = {\n",
    "        'istdaten':'com490/data/sbb/parquet/istdaten/',\n",
    "        'shapes':'com490/data/geo/parquet/',\n",
    "        'stops': 'com490/data/sbb/parquet/timetable/stops'\n",
    "}\n",
    "\n",
    "bucket = config.get('default', 'bucket')\n",
    "s3endpoint = config.get('default', 'endpoint')\n",
    "s3key=creds.get('default', 'key')\n",
    "s3secret=creds.get('default', 'secret')\n",
    "\n",
    "metadata={}\n",
    "for k,v in s3_prefixes.items():\n",
    "    metadata[k] = get_metadata(bucket, v, s3endpoint, s3key, s3secret)\n",
    "    print(f'{k}: s3://{bucket}/{metadata[k]}')\n",
    "\n",
    "conn = duckdb.connect()\n",
    "conn.execute(\"INSTALL 'httpfs';\")\n",
    "conn.execute(\"INSTALL 'iceberg';\")\n",
    "conn.execute(\"INSTALL 'spatial';\")\n",
    "conn.execute(\"LOAD 'spatial';\")\n",
    "conn.execute(\"LOAD 'httpfs';\")\n",
    "conn.execute(\"LOAD 'iceberg';\")\n",
    "\n",
    "conn.execute(f\"\"\"\n",
    "CREATE SECRET secret3 (\n",
    "    TYPE S3,\n",
    "    KEY_ID '{s3key}',\n",
    "    SECRET '{s3secret}',\n",
    "    ENDPOINT '{s3endpoint}',\n",
    "    URL_STYLE 'path',\n",
    "    USE_SSL true,\n",
    "    REGION 'ZH'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee346ed-ed58-429b-b59b-206bd67109d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = conn.execute(f\"\"\"FROM which_secret('s3://{bucket}/{metadata[\"stops\"]}', 's3')\"\"\")\n",
    "print(ans.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ecbf2-13fd-485e-ac8d-2d7896bb676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = conn.execute(f\"\"\"SELECT COUNT(*) FROM iceberg_scan('s3://{bucket}/{metadata[\"stops\"]}')\"\"\")\n",
    "print(ans.fetchone())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d505a-b541-4be6-9b56-bd696ffc8c77",
   "metadata": {},
   "source": [
    "Prepare the SQL query:\n",
    "- _stop_: names and geolocations SBB stops\n",
    "- _shape_: geospatial shapes of the swiss city boundaries (administrative zones)\n",
    "- _geo_tagged_stop_: table derived from _stop_ and _shape_ placing stops in their respective cities\n",
    "- _geo_tagged_istdaten_: actual arrival and departure delays, with information about day of week, hour and city containing the stop (from geo_tagged_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b71cab-2e13-48a0-82a7-aabc18fd08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregatePercentiles = f\"\"\"\n",
    "WITH\n",
    "        stop  AS (\n",
    "                SELECT TRY_CAST(stop_id[:7] as INTEGER) as bpuic, stop_lat, stop_lon\n",
    "                FROM iceberg_scan('s3://{bucket}/{metadata[\"stops\"]}')\n",
    "                WHERE year=2024 AND month=9 AND day=9\n",
    "        ),\n",
    "\n",
    "        shape AS (\n",
    "                SELECT ST_GeomFromWKB(wkb_geometry) as geometry, name\n",
    "                FROM iceberg_scan('s3://{bucket}/{metadata[\"shapes\"]}')\n",
    "                WHERE level='city'\n",
    "        ),\n",
    "\n",
    "        geo_tagged_stop AS (\n",
    "                SELECT stop.bpuic, stop.stop_lat, stop.stop_lon, shape.name\n",
    "                FROM stop JOIN shape ON ST_Contains(shape.geometry, ST_Point(stop.stop_lon, stop.stop_lat))\n",
    "        ),\n",
    "\n",
    "        geo_tagged_istdaten AS (\n",
    "                SELECT dayofweek(iceberg_scan_data.arr_actual) as day_week, hour(iceberg_scan_data.arr_actual) as hour_day,\n",
    "                       date_diff('second', iceberg_scan_data.arr_time, iceberg_scan_data.arr_actual) as arr_delay, date_diff('second',iceberg_scan_data.dep_time, iceberg_scan_data.dep_actual) as dep_delay, geo_tagged_stop.name\n",
    "                FROM iceberg_scan('s3://{bucket}/{metadata[\"istdaten\"]}')\n",
    "                JOIN geo_tagged_stop ON geo_tagged_stop.bpuic = iceberg_scan_data.bpuic\n",
    "        )\n",
    "SELECT AVG(arr_delay) as arr_delay, AVG(dep_delay) as dep_delay, COUNT(*) as num,\n",
    "           approx_quantile(arr_delay, 0.25), approx_quantile(arr_delay,0.5), approx_quantile(arr_delay,0.75), hour_day, name\n",
    "       FROM geo_tagged_istdaten WHERE day_week >= 1 AND day_week <= 5 GROUP BY name,hour_day ORDER BY name,hour_day\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed2237-0b0c-46f7-8530-aa6557299c11",
   "metadata": {},
   "source": [
    "Execute the query and get the results.\n",
    "\n",
    "You can for instance iterate the cursor and write the rows to file or create a pandas DataFrame.\n",
    "\n",
    "```\n",
    "%%time\n",
    "from contextlib import closing\n",
    "import pandas as pd\n",
    "with closing(conn.cursor()) as cur:\n",
    "    cur.execute(aggregatePercentiles)\n",
    "    columns = [col[0] for col in cur.description]\n",
    "    df = pd.DataFrame(cur, columns=columns)\n",
    "```\n",
    "\n",
    "You can also directly use the pandas.read_sql_query directly. If using this second option you will need to suppress a warning, because pandas does not know that duckdb is DBAPI2 compliant\n",
    "\n",
    "⚠️ because duckdb runs locally, make sure that you have sufficient CPU and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb3626-e7cf-4741-8a62-857881f2678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # Catch UserWarning: pandas only supports SQLAlchemy\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    df = pd.read_sql_query(aggregatePercentiles, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e5bdf-73d2-45ff-a8da-34385f23c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223be1e-c43b-4b94-a781-af94ca5cca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94129e3c-f829-42b1-8b28-ea51d5b376ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
